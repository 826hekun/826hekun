# 安装和加载必要的包
install.packages(c("randomForest", "glmnet", "caret", "xgboost", "e1071"))
library(randomForest)
library(glmnet)
library(caret)
library(xgboost)
library(e1071)

# 假设您已经加载了数据集，并且数据集已经预处理完毕
# 以下代码使用内置的iris数据集作为示例

# 随机森林 (Random Forest)
data(iris)
target <- iris$Species
features <- iris[, -5]
rf_model <- randomForest(target ~ ., data=features, ntree=100)
print(rf_model)

# Lasso (最小绝对收缩和选择算子)
set.seed(123)
trainIndex <- createDataPartition(target, p=0.7, list=FALSE)
trainX <- features[trainIndex, ]
trainY <- target[trainIndex]
cv_model <- cv.glmnet(trainX, trainY, alpha=1, family="multinomial")
lasso_model <- glmnet(trainX, trainY, alpha=1, lambda=cv_model$lambda.min, family="multinomial")
print(lasso_model)

# 岭回归 (Ridge)
ridge_model <- glmnet(trainX, trainY, alpha=0, lambda=seq(0, 0.1, length=100))
print(ridge_model)

# 弹性网络 (Elastic Net)
enet_model <- glmnet(trainX, trainY, alpha=0.5, lambda=seq(0, 0.1, length=100))
print(enet_model)

# 支持向量机 (Support Vector Machine)
svm_model <- svm(target ~ ., data=features, method="C-classification", kernel="linear")
print(svm_model)

# 梯度提升机 (Gradient Boosting Machine)
gbm_model <- gbm(target ~ ., data=features, distribution="multinomial", n.trees=100, shrinkage=0.05)
print(gbm_model)

# XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(features), label = as.numeric(target))
xgb_model <- xgb.train(data = dtrain, max_depth = 3, eta = 0.1, nthread = 2, nround = 100, objective = "multi:softmax")
print(xgb_model)

# 朴素贝叶斯 (Naive Bayes)
nb_model <- naiveBayes(target ~ ., data=features)
print(nb_model)

